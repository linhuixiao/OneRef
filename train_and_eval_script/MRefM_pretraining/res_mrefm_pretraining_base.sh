# MRefM pretraining
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=8 --master_port 28887 --use_env pretrain_oneref_with_mrefm.py --num_workers 4 --epochs 10   --batch_size 64 --lr 0.00025  --lr_scheduler cosine --aug_crop --aug_scale --aug_translate    --imsize 384 --max_query_len 64  --model beit3_base_patch16_384 --task grounding  --dataset mixup --frozen_backbone                        --enable_mrefm --enable_dynamic_mim --dynamic_mask_ratio 0.75 --sentencepiece_model /path_to/checkpoint/beit3/beit3.spm --tokenizer_weight /path_to/checkpoint/beit3/vqkd_encoder_base_decoder_3x768x12_clip-d5036aa7.pth --finetune /path_to/checkpoint/beit3/beit3_base_indomain_patch16_224.pth  --data_root /path_to_image_data --split_root /path_to/ref_data_shuffled/mixup_with_refc  --output_dir /path_to/outputs/oneref/v001/pretrain;
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=8 --master_port 28887 --use_env pretrain_oneref_with_mrefm.py --num_workers 4 --epochs 100  --batch_size 32 --lr 0.00005  --lr_scheduler cosine --aug_crop --aug_scale --aug_translate    --imsize 384 --max_query_len 64  --model beit3_base_patch16_384 --task grounding  --dataset mixup                                          --enable_mrefm --enable_dynamic_mim --dynamic_mask_ratio 0.75 --sentencepiece_model /path_to/checkpoint/beit3/beit3.spm --tokenizer_weight /path_to/checkpoint/beit3/vqkd_encoder_base_decoder_3x768x12_clip-d5036aa7.pth --finetune /path_to/outputs/oneref/v001/pretrain/checkpoint.pth           --data_root /path_to_image_data --split_root /path_to/ref_data_shuffled/mixup_with_refc  --output_dir /path_to/outputs/oneref/v002/pretrain;
